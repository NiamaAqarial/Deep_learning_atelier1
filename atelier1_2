# =============================================================================
#                        PARTIE 2 – CLASSIFICATION MULTI-CLASSE
# =============================================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split, ParameterGrid
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (mean_squared_error, r2_score,
                             accuracy_score, classification_report, f1_score)
import warnings
warnings.filterwarnings('ignore')
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"Memory Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\n")

print("\n"*3 + "="*80)
print("PARTIE 2 : Classification multi-classe – Maintenance prédictive")
print("="*80)

df = pd.read_csv("predictive_maintenance.csv", header=0)

print("Shape :", df.shape)
print(df.head())

# 1. Nettoyage & pré-processing
df = df.drop(['UDI', 'Product ID'], axis=1)
df['Type'] = df['Type'].map({'L':0, 'M':1, 'H':2})

print("\nRépartition des classes (très déséquilibrée) :")
print(df['Failure Type'].value_counts())

# 2. EDA Classification
plt.figure(figsize=(14,5))
plt.subplot(1,2,1)
sns.countplot(x='Failure Type', data=df)
plt.title('Distribution des types de panne'); plt.xticks(rotation=45)

plt.subplot(1,2,2)
numeric_cols = df.drop(['Target', 'Failure Type'], axis=1)
sns.heatmap(numeric_cols.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Corrélation des variables numériques')
plt.tight_layout(); plt.show()

# 3. Équilibrage manuel
from sklearn.preprocessing import LabelEncoder
from collections import Counter

print("\nAvant équilibrage :", Counter(df['Failure Type']))

# On encode d'abord les labels textuels en nombres entiers
le = LabelEncoder()
df['Failure Type Encoded'] = le.fit_transform(df['Failure Type'])

# Maintenant on utilise la colonne encodée comme cible
X_cls = df.drop(['Target', 'Failure Type', 'Failure Type Encoded'], axis=1).values
y_cls = df['Failure Type Encoded'].values   # ← entiers : 0,1,2,3,4,5

print("Classes après encodage :", le.classes_)
print("Exemple y_cls :", y_cls[:10])

# Fonction d'oversampling (inchangée)
def oversample_balance(X, y):
    X = np.array(X); y = np.array(y)
    counts = Counter(y)
    max_count = max(counts.values())
    X_bal, y_bal = [], []
    for cls in counts:
        idx = np.where(y == cls)[0]
        chosen = np.random.choice(idx, max_count, replace=True)
        X_bal.extend(X[chosen])
        y_bal.extend(y[chosen])
    return np.array(X_bal), np.array(y_bal)

X_bal, y_bal = oversample_balance(X_cls, y_cls)
print("Après équilibrage manuel :", Counter(y_bal))

# Sauvegarde du LabelEncoder pour plus tard (décodage des prédictions)
import joblib
joblib.dump(le, 'label_encoder_maintenance.pkl')  # optionnel mais propre

# 4. Split + scaling
X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(
    X_bal, y_bal, test_size=0.2, random_state=42, stratify=y_bal)

scaler_c = StandardScaler()
X_train_c = scaler_c.fit_transform(X_train_c)
X_test_c  = scaler_c.transform(X_test_c)

# Move tensors to GPU
X_train_ct = torch.FloatTensor(X_train_c).to(device)
X_test_ct  = torch.FloatTensor(X_test_c).to(device)
y_train_ct = torch.LongTensor(y_train_c).to(device)
y_test_ct  = torch.LongTensor(y_test_c).to(device)

# DataLoader for classification
train_dataset_cls = list(zip(torch.FloatTensor(X_train_c), torch.LongTensor(y_train_c)))
train_loader_cls = DataLoader(train_dataset_cls, batch_size=128, shuffle=True)

# 5. Modèle classification
class Classifier(nn.Module):
    def __init__(self, dropout=0.4):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(6, 256),
            nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(dropout),
            nn.Linear(256, 128),
            nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(dropout),
            nn.Linear(128, 64), nn.ReLU(),
            nn.Linear(64, 6)
        )
    def forward(self, x): 
        return self.net(x)
# 6. GridSearch classification
print("\nGridSearch pour le classifieur...")
grid_cls = ParameterGrid({
    'lr': [0.001, 0.0005],
    'dropout': [0.3, 0.5]
})

best_acc = 0
best_clf = None

for p in grid_cls:
    model = Classifier(dropout=p['dropout']).to(device)
    opt = optim.Adam(model.parameters(), lr=p['lr'], weight_decay=1e-5)
    crit = nn.CrossEntropyLoss()
    
    for epoch in range(60):
        model.train()
        for xb,yb in train_loader_cls:
            xb, yb = xb.to(device), yb.to(device)
            opt.zero_grad()
            loss = crit(model(xb), yb)
            loss.backward()
            opt.step()
    
    model.eval()
    with torch.no_grad():
        acc = (model(X_test_ct).argmax(1) == y_test_ct).float().mean().item()
    print(f"Testé lr={p['lr']} dropout={p['dropout']} → Accuracy = {acc:.4f}")
    if acc > best_acc:
        best_acc = acc
        best_clf = model
        best_p_cls = p

print(f"\nMeilleur classifieur → {best_p_cls} | Accuracy = {best_acc:.4f}")

# 7. Entraînement final + courbes + métriques
optimizer_final = optim.Adam(best_clf.parameters(), lr=best_p_cls['lr'], weight_decay=1e-5)
criterion = nn.CrossEntropyLoss()

train_losses_c, test_losses_c = [], []
train_accs_c,   test_accs_c   = [], []

for epoch in range(120):
    best_clf.train()
    correct = total = 0
    epoch_loss = 0
    for xb,yb in train_loader_cls:
        xb, yb = xb.to(device), yb.to(device)
        optimizer_final.zero_grad()
        out = best_clf(xb)
        loss = criterion(out, yb)
        loss.backward()
        optimizer_final.step()
        epoch_loss += loss.item()
        correct += (out.argmax(1)==yb).sum().item()
        total += yb.size(0)
    train_accs_c.append(correct/total)
    train_losses_c.append(epoch_loss/len(train_loader_cls))
    
    best_clf.eval()
    with torch.no_grad():
        out_test = best_clf(X_test_ct)
        test_loss = criterion(out_test, y_test_ct).item()
        test_acc = (out_test.argmax(1)==y_test_ct).float().mean().item()
    test_losses_c.append(test_loss)
    test_accs_c.append(test_acc)
    
    if (epoch+1)%20==0:
        print(f"Epoch {epoch+1:3d} → Train Acc: {train_accs_c[-1]:.4f} | Test Acc: {test_accs_c[-1]:.4f}")

# Courbes
plt.figure(figsize=(14,5))
plt.subplot(1,2,1)
plt.plot(train_losses_c, label='Train Loss')
plt.plot(test_losses_c, label='Test Loss')
plt.title('Loss / Epochs – Classification'); plt.legend(); plt.grid()

plt.subplot(1,2,2)
plt.plot(train_accs_c, label='Train Accuracy')
plt.plot(test_accs_c, label='Test Accuracy')
plt.title('Accuracy / Epochs – Classification'); plt.legend(); plt.grid()
plt.show()

# Métriques détaillées
with torch.no_grad():
    y_pred = best_clf(X_test_ct).argmax(1).cpu().numpy()
y_test_cpu = y_test_ct.cpu().numpy()

# Décodage pour un beau rapport
y_pred_labels = le.inverse_transform(y_pred)
y_test_labels = le.inverse_transform(y_test_cpu)

print("\nRapport de classification")
print(classification_report(y_test_labels, y_pred_labels, digits=4))

print(f"\n{'='*80}")
print(f"Training complete! Device used: {device}")
print(f"{'='*80}")